dataframe.plot()으로 바로 시각화 가능

## 경사 하강법

- 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 최적화(Optimization) 알고리즘
- 손실(비용) 함수를 최소화하기 위해 반복적으로 파라미터를 조정
- 선형 회귀의 경우 손실함수(MSE)를 최소화 하는 파라미터 w1, w0에 대해 함수의 현재 기울기를 계산한다. 기울기가 감소하는 방향으로 진행하고, 기울기가 0이 되면 최솟값에 도달한 것

### 경사 하강법의 장점

- 함수가 너무 복잡해 미분 계수를 구하기 어려운 경우
- ‘데이터를 기반으로 알고리즘이 스스로 학습한다’는 머신러닝의 개념을 가능하게 해준 핵심 기법 중 하나

### 경사 하강법 주의사항

- 잘못된 하이퍼 파라미터 설정 시, 최적값에 도달하지 못할 수 있다.
- 학습률이 너무 크면 손실함수가 발산할 수 있으며, 반대로 너무 작으면 최적값에 도달하기까지 시간이 너무 오래 걸릴 수 있다.
- 최적값에 도달했는지 계속해서 관찰해야 한다.
- 비선형 함수의 경우, 손실함수가 복잡하고 특정 값에서 이를 지나치게 발산할 수 있다.
- 비블록 함수일 경우 최적값을 찾지 못하고 지역 최적값을 구할 수 있다.

### 학습률(Learning Rate)

- 매 계산(step)마다 적용되는 이동 거리
- 기울기 갱신의 폭 결정
- 하이퍼 파라미터로 조절 가능함
- 기울기가 충분히 작아질 때까지 갱신하다가 기울기가 평평한 곳에 도달하면 갱신을 종료
- 기울기 갱신 횟수, 기울기 최솟값을 사전에 지정(하이퍼 파라미터)

### 배치(batch)

- 1회의 경사 업데이트(스텝)에 사용되는 데이터 집합
- 이 때 사용되는 데이터 집합의 개수를 배치 크기(배치 사이즈)라고 함
- 예) 전체 데이터 세트: 100개, 배치 사이즈: 20,
배치 개수: 5, 경사(기울기) 업데이트 수: 5회

### 에폭(epoch)

- 전체 데이터들을 한 번 모두 사용하는 것
- 일반적으로 경사하강법은 수십, 수백 번 이상 에폭을 수행
- 예) 전체 데이터 세트: 100개, 배치 사이즈: 20, 에폭: 1000
에폭 1회 → 배치 개수: 5, 경사(기울기)업데이트 수 : 5회
에폭 1000회 → 경사 업데이트 수(학습횟수): 5000

### 경사하강법의 종류

- 배치 경사하강법
    - 매 스텝에서 훈련 데이터 전체를 사용(배치의 크기: 훈련 데이터 전부)하여 계산한 후 최적의 한 스텝을 나아감
    - 매우 큰 훈련 세트에서는 아주 느림
    - 한 개의 배치에 전체 학습 데이터가 모두 들어간다.

- 확률적 배치 경사하강법(SGD)
    - 매 스텝에서 한 개의 샘플(훈련 데이터)을 무작위로 선택하고 그 하나의 샘플에 대한 그래디언트(기울기)를 계산한다.
    - 한 개의 배치에 임의의 학습 데이터 1개만 들어간다.
    - 매 스텝에서 다루어야 할 데이터가 매우 작으므로(배치의 크기 1) 처리 속도가 빠름
    - 빠르게 최적점을 찾을 수 있지만 정확도는 낮아짐
    - 매우 큰 훈련 데이터도 학습 가능함
    - 데이터가 들어오는 즉시 가중치를 갱신할 수 있어 온라인 학습이 가능하며 즉각적인 시스템 대응이 가능
- 미니 배치 경사하강법
    - 확률적 경사하강법 및 배치 경사하강법의 절충안
    - 각 스탭에서 전체 훈련 세트(배치 경사하강법)이나 하나의 샘플(확률적 경사 하강법)을 기반으로 기울기를 계산하지 않고, 미니 배치라고 부르는 임의의 샘플 세트를 기반으로 기울기를 계산함
    - 1개의 배치에 임의의 학습 데이터 여러개가 들어감
    - 배치 경사하강법보다 효율적이고, 확률적 경사하강법 보다 노이즈가 적다.
    - 행렬 연산에 최적화된 하드웨어(GPU)에서 성능이 향상됨

## 특성 스케일링

- 데이터 변환 중 가장 중요한 변환 중 하나
- 대부분의 머신러닝 알고리즘은 입력 숫자 특성들의 스케일이 많이 다르면 잘 작동하지 않음
(Decision Tree 예외)
- 정규화
    - 모든 값이 0~1 사이에 들도록 범위를 조정(feature_range로 조정 가능)
    - sklearn.preprocessing.MinMaxScaler
- 표준화
    - 평균을 뺀 후 표준편차로 나누어 평균 0, 분산 1이 되는 분포로 전환
    - 각 특성값이 0에서 표준 편차의 몇 배만큼 떨어져 있는가
    - Min-max 스케일링과 달리 표준화는 범위의 상한과 하한이 없음
    - 신경망의 경우 입력값의 범위를 0~1로 기대함
    - 표준화는 이상치에 영향을 덜 받음(vs min-max 스케일링)
    - sklearn.preprocessing.StandardScaler
- 타겟 스케일링은 필요하지 않음

## 다항 회귀

- 독립변수가 단항식이 아닌 2차, 3차 등으로 표현되는 것
- 데이터가 단순한 직선이 아닌 복잡한 형태인 경우, 산점도 상의 관측값을 통과하는 추세선을 그려서
n-1 개의 굴절이 관찰되면 n차 다항식으로 모델링
- 독립변수와 종속 변수의 관계를 다차 다항식으로 표현
    - y = w0 + w1x + w2x^2 + w3x^3 + wnx^n
- 다중 선형회귀(여러 개의 독립변수)의 특별한 형태
    - y = w0 + w1x1 + w2x2 + wnxn (^ 아님 )
- 차수의 변환 후 다중 선형회귀와 같은 방식으로 비선형 관계를 모델링
- 차수가 증가하면 곡선 모델이 될 때 다항회귀라 함
- 차수가 높아질 수록 새로운 속성이 기하급수적으로 늘어남
    - 계산 시간이 오래걸리고 과적합 문제가 발생 가능

### 사이킷런의 변환기

- from sklearn.preprocessing import PolynomialFeatures
- fit()
    - 새롭게 만들 특성의 조합(기준)을 찾음
- transform()
    - 실제로 데이터 변한
- fit_transform()
    - 위 두 동작을 한 번에

### 선형 회귀 유형

- 단일 단항, 다중 단항, 단일 다항, 다중 다항

## 모델 최적화

- 최적화
    - 가능한 훈련데이터에서 최고의 성능을 얻으려고 모델을 조정하는 과정(머신러닝의 학습)
- 일반화
    - 훈련된 모델이 학습한 적 없는 데이터에서 얼마나 잘 수행되는지 판별